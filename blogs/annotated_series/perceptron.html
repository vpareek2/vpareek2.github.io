<!DOCTYPE HTML>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>The Annotated Neural Network: The Perceptron - Veer Pareek</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link href="../../style.css" rel="stylesheet">
    <style>
        body, td, th, tr, p, a {
            font-family: 'Lora', serif !important;
            font-size: 16px !important;
        }
        h1, h2, h3, h4, h5, h6 {
            font-family: 'Lora', serif !important;
        }
        pre {
            background-color: #f6f8fa;
            padding: 16px;
            overflow: auto;
            font-size: 85%;
            line-height: 1.45;
            border-radius: 6px;
            font-family: monospace !important;
        }
    </style>
</head>
<body>
    <!-- Navigation Bar -->
    <nav>
        <div class="nav-left">
            <a href="../../index.html" class="name">Veer Pareek</a>
        </div>
        <ul>
            <li><a href="../../index.html">Home</a></li>
            <li><a href="../../coursework.html">Coursework</a></li>
            <li><a href="../../projects.html">Projects</a></li>
            <li><a href="../../blog.html" class="active">Blog</a></li>
        </ul>
    </nav>
    
    <!-- Blog Post Content -->
    <div class="content">
        <h1 class="post-title" style="text-align: center;">The Annotated Neural Network</h1>
        <h2 class="post-subtitle" style="text-align: center; font-family: 'Lora', serif; font-size: 28px; font-weight: normal; color: #000000; margin-top: 0; margin-bottom: 20px;">The Perceptron</h2>

        <div class="post-meta" style="text-align: center; font-family: 'Lora', serif; font-size: 18px; font-weight: bold; color: #000000;">Veer Pareek</div>
        <div class="post-content">
            <p>The Perceptron is the simplest form of a Neural Network. This post presents an annotated version of the architecture in a line-by-line implementation in python.</p>

            <h2>Background</h2>
            <p>The perceptron, also referred to as the McCulloch-Pitts neuron, is a machine learning algorithm for <a href="https://en.wikipedia.org/wiki/Supervised_learning">supervised learning</a>. Specifically, this algorithm works for <a href="https://en.wikipedia.org/wiki/Binary_classification">binary classification</a>. It is also a type of <a href="https://en.wikipedia.org/wiki/Linear_classifier">linear classifier</a>.</p>

            <p>The Perceptron was originally invented in 1943 by Warren McCulloch and Walter Pitts. The first hardware implementation was built in 1957 by Frank Rosenblatt. The Mark I Perceptron had 3 layers:</p>

            <ul>
                <li>An array of 400 photocells arranged in a 20x20 grid, named S-units</li>
                <li>A hidden layer of 512 perceptrons, named A-units</li>
                <li>An output layer of 8 perceptrons, named R-units</li>
            </ul>

            <p>Each S-unit can connect up to 40 A-units, stochastically, to eliminate any initial bias. For more details on the hardware implementation, refer to the <a href="https://www.ling.upenn.edu/courses/cogs501/Rosenblatt1958.pdf">original paper</a>.</p>

            <h2>Model Architecture</h2>
            <p>The modern Perceptron consists of the following components:</p>

            <h3>Input Layer</h3>
            <ul>
                <li>Consists of <em>n</em> input nodes where each node represents a feature of the input data</li>
                <li>A bias input, which always has a value of 1</li>
            </ul>

            <h3>Weights</h3>
            <ul>
                <li>Each input has an associated weight</li>
                <li>Weights are adjustable and learned during training</li>
            </ul>

            <h3>Single Neuron</h3>
            <ul>
                <li>One output neuron that computes the weighted sum of inputs</li>
                <li>Applies an activation function to weighted sum</li>
            </ul>

            <h3>Activation Function</h3>
            <ul>
                <li>A threshold function</li>
            </ul>

            <h3>Output</h3>
            <ul>
                <li>A single scalar value, either 0 or 1</li>
            </ul>

            <p>You may notice that this modern perceptron differs from Rosenblatt's original implementation. Specifically the lack of A-units. This is because it was proven that the A-units did not provide significant advantages for the types of problems perceptrons are meant to solve. Removing the A-units reduced computational complexity with sacrificing the perceptron's fundamental capabilities. Though, the concept of A-units (intermediate layers) is key for Multi-Layer Perceptrons and other Neural Network variants.</p>

            <h2>The Annotated Implementation</h2>
            <p>This is an implementation written from scratch. While a single-layer perceptron is rarely used in practice, in general, it is better to use libraries such as PyTorch or TensorFlow for deep learning as they are highly optimized.</p>

            <h3>Dependencies</h3>
            <p>Being an implementation from scratch, numpy is the only dependency. NumPy is a library which adds support for arrays and matrices with functions to operate on them.</p>
            <pre><code>import numpy as np</code></pre>

            <h3>Initialization</h3>
            <p>The <code>Perceptron</code> class is initialized with four parameters:</p>
            <ul>
                <li><code>num_features</code> : the number of inputs into the network</li>
                <li><code>learning_rate</code> : scalar value that controls the magnitude of weight updates during training</li>
                <li><code>num_epochs</code> : the number of iterations in training</li>
                <li><code>weights</code> : numerical values that determine the importance and influence of each input feature, these are adjusted during training</li>
            </ul>

            <pre><code>
class Perceptron:
    def __init__(self, num_features, learning_rate, num_epochs):
        self.num_features = num_features
        self.learning_rate = learning_rate
        self.num_epochs = num_epochs
        self.weights = np.random.randn(num_features + 1)
            </code></pre>

            <h3>Activation Function</h3>
            <p>The activation function in a single layer perceptron is the threshold function. Since it is meant for binary classification, it checks whether <code>x</code> is above a certain threshold and returns 1 if so, and 0 if not. It is mathematically represented as:</p>

            <div style="font-family: 'Times New Roman', Times, serif; font-size: 16px;">
                <p>
                    f(x) = 
                    {
                    <span style="display: inline-block; vertical-align: middle;">
                        <span style="display: block; border-bottom: 1px solid black; padding-bottom: 2px;">1 &nbsp; if x â‰¥ 0</span>
                        <span style="display: block; padding-top: 2px;">0 &nbsp; if x < 0</span>
                    </span>
                    }
                </p>
            </div>

            <p>This function is implemented in the Perceptron class as:</p>
            <pre><code>
def activate(self, x):
    return 1 if x >= 0 else 0
            </code></pre>

            <h3>Prediction</h3>
            <p>The prediction function takes in one parameter:</p>
            <ul>
                <li><code>inputs</code> : the input data for prediction</li>
            </ul>

            <p>The following line does a few things:</p>
            <pre><code>summation = np.dot(np.insert(inputs, 0, 1), self.weights)</code></pre>

            <ul>
                <li><code>np.insert(inputs, 0, 1)</code> adds a 1 at the beginning of the input array which represents the bias term</li>
                <li><code>self.weights</code> are the current weights of the perceptron, including the bias</li>
                <li><code>np.dot</code> calculates the dot product between the modified inputs and the weights</li>
            </ul>

            <p>Finally, the summation is passed through the threshold activation function to return 1 or 0:</p>
            <pre><code>return self.activate(summation)</code></pre>

            <p>The full predict function looks like this:</p>
            <pre><code>
def predict(self, inputs):
    summation = np.dot(np.insert(inputs, 0, 1), self.weights)
    return self.activate(summation)
            </code></pre>

            <h3>Training</h3>
            <p>Model training, in general, is the process of teaching a machine learning algorithm to make accurate predictions based on the input data. Our training function looks like so:</p>
            <pre><code>def train(self, X, y):</code></pre>
            <p>where <code>X</code> is the input data and <code>y</code> represents the target labels.</p>

            <p>In the context of the single layer perceptron, the training loop consists of 3 main steps:</p>

            <ol>
                <li><i>Initialization:</i> The Perceptron starts with randomly initialized weights set earlier in the <code>__init__</code> function, and is not initialized with the bias term:
                <pre><code>X = np.insert(X, 0, 1, axis=1)  # Add bias term</code></pre>
                </li>

                <li><i>Training Loop:</i> The main training loop contains multiple steps, including:
                    <ul>
                        <li>Iterating through epochs and create total error:
                        <pre><code>
for epoch in range(self.num_epochs):
    total_error = 0
                        </code></pre>
                        </li>
                        <li>Processing each sample, iterating over each training example:
                        <pre><code>for inputs, label in zip(X, y):</code></pre>
                        </li>
                        <li>Making predictions using our predictions function:
                        <pre><code>prediction = self.predict(inputs[1:])</code></pre>
                        </li>
                        <li>Calculating errors by taking the difference between the true label and prediction:
                        <pre><code>
error = label - prediction
total_error += abs(error)
                        </code></pre>
                        </li>
                        <li>Updating weights based on the error, learning rate, and input values:
                        <pre><code>self.weights += self.learning_rate * error * inputs</code></pre>
                        </li>
                    </ul>
                </li>

                <li><i>Evaluation and update:</i> Evaluations and updates are not as relevant in a single-layer perceptron. Here, it is just printing information and checking for perfect classification, which is a simple form of early-stopping. But in a more complex neural network, this step would include things like:
                    <ul>
                        <li>Backpropagation</li>
                        <li>Gradient Descent</li>
                        <li>More advanced early stopping</li>
                        <li>Learning rate adjustment</li>
                        <li>Regularization</li>
                        <li>Normalization</li>
                        <li>Checkpointing</li>
                    </ul>
                </li>
            </ol>

            <p>The full train function looks like this:</p>
            <pre><code>
def train(self, X, y):
    X = np.insert(X, 0, 1, axis=1)  # Add bias term
    for epoch in range(self.num_epochs):
        total_error = 0
        for inputs, label in zip(X, y):
            prediction = self.predict(inputs[1:])
            error = label - prediction
            total_error += abs(error)
            self.weights += self.learning_rate * error * inputs

        # Print training progress
        if epoch % 100 == 0:
            print(f"Epoch {epoch}: weights = {self.weights}, total error = {total_error}")

        # Early stopping if perfect classification
        if total_error == 0:
            print(f"Converged at epoch {epoch}")
            break
            </code></pre>

            <h3>Usage</h3>
            <p>The Single-Layer Perceptron is a very limited architecture. It is difficult to find a problem that this model will excel at. To demonstrate some of the limitations of the model, we can look at the XOR problem.</p>

            <p>The XOR problem, also referred to as the "exclusive or" problem, is the problem of using a neural network to predict the outputs of XOR logic gates given two binary inputs. It is a problem because the XOR function is not linearly separable, meaning you can't draw a single straight line on the 2D plot that separates 1s from 0s.</p>

            <p>Imagine plotting the following points on a 2D graph:</p>

            <ul>
                <li>(0,0) -> Output 0</li>
                <li>(0,1) -> Output 1</li>
                <li>(1,0) -> Output 1</li>
                <li>(1,1) -> Output 0</li>
            </ul>

            <p>There is no way to draw a single straight line that separates the 1s from 0s. This matters in the context of single-layer perceptrons because it tries to find a linear decision boundary (imagine a straight line on a 2D plot) to separate classes. This is solvable by multi-layer neural networks.</p>

            <p>Here is how the problem is set up in code:</p>
            <pre><code>
if __name__ == "__main__":
    # Input data
    X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])

    # AND problem
    y_and = np.array([0, 0, 0, 1])
    perceptron_and = Perceptron(num_features=2, learning_rate=0.1, num_epochs=1000)
    perceptron_and.train(X, y_and)
    
    print("\nAND function results:")
    for inputs in X:
        print(f"Input: {inputs}, Prediction: {perceptron_and.predict(inputs)}")

    # OR problem
    y_or = np.array([0, 1, 1, 1])
    perceptron_or = Perceptron(num_features=2, learning_rate=0.1, num_epochs=1000)
    perceptron_or.train(X, y_or)
    
    print("\nOR function results:")
    for inputs in X:
        print(f"Input: {inputs}, Prediction: {perceptron_or.predict(inputs)}")

    # XOR problem
    y_xor = np.array([0, 1, 1, 0])
    perceptron_xor = Perceptron(num_features=2, learning_rate=0.1, num_epochs=1000)
    perceptron_xor.train(X, y_xor)
    
    print("\nXOR function results:")
    for inputs in X:
        print(f"Input: {inputs}, Prediction: {perceptron_xor.predict(inputs)}")
            </code></pre>

            <p>This is the output:</p>
            <pre><code>
Epoch 0: weights = [ 0.00359545  0.02398612 -0.45497649], total error = 2
Converged at epoch 11

AND function results:
Input: [0 0], Prediction: 0
Input: [0 1], Prediction: 0
Input: [1 0], Prediction: 0
Input: [1 1], Prediction: 1
Epoch 0: weights = [-2.52805087 -0.31402282 -0.41771306], total error = 3
Converged at epoch 8

OR function results:
Input: [0 0], Prediction: 0
Input: [0 1], Prediction: 1
Input: [1 0], Prediction: 1
Input: [1 1], Prediction: 1
Epoch 0: weights = [-0.46176641  1.40984421  0.78214246], total error = 1
Epoch 100: weights = [ 0.03823359 -0.09015579 -0.01785754], total error = 4
Epoch 200: weights = [ 0.03823359 -0.09015579 -0.01785754], total error = 4
Epoch 300: weights = [ 0.03823359 -0.09015579 -0.01785754], total error = 4
Epoch 400: weights = [ 0.03823359 -0.09015579 -0.01785754], total error = 4
Epoch 500: weights = [ 0.03823359 -0.09015579 -0.01785754], total error = 4
Epoch 600: weights = [ 0.03823359 -0.09015579 -0.01785754], total error = 4
Epoch 700: weights = [ 0.03823359 -0.09015579 -0.01785754], total error = 4
Epoch 800: weights = [ 0.03823359 -0.09015579 -0.01785754], total error = 4
Epoch 900: weights = [ 0.03823359 -0.09015579 -0.01785754], total error = 4

XOR function results:
Input: [0 0], Prediction: 1
Input: [0 1], Prediction: 1
Input: [1 0], Prediction: 0
Input: [1 1], Prediction: 0
            </code></pre>

            <p>The results clearly demonstrate the capabilities and limitations of single-layer perceptrons. The model successfully learned the AND and OR functions, converging quickly (at epochs 11 and 8 respectively) and making correct predictions for all inputs. However, it failed to solve the XOR problem. For XOR, the perceptron did not converge even after 1000 epochs, with the total error remaining constant at 4 from epoch 100 onwards. It made incorrect predictions for [0,0] and [1,0] inputs, illustrating its inability to create the non-linear decision boundary required for XOR classification. This outcome confirms that single-layer perceptrons cannot solve non-linearly separable problems like XOR, highlighting the need for more complex neural network architectures in such cases.</p>

            <br><h2>Full implementation</h2>
            <pre><code>
import numpy as np

class Perceptron:
    def __init__(self, num_features, learning_rate=0.1, num_epochs=1000):
        self.num_features = num_features
        self.learning_rate = learning_rate
        self.num_epochs = num_epochs
        self.weights = np.random.randn(num_features + 1)  # Random initialization

    def activate(self, x):
        return 1 if x >= 0 else 0

    def predict(self, inputs):
        summation = np.dot(np.insert(inputs, 0, 1), self.weights)
        return self.activate(summation)

    def train(self, X, y):
        X = np.insert(X, 0, 1, axis=1)  # Add bias term
        for epoch in range(self.num_epochs):
            total_error = 0
            for inputs, label in zip(X, y):
                prediction = self.predict(inputs[1:])
                error = label - prediction
                total_error += abs(error)
                self.weights += self.learning_rate * error * inputs

            # Print training progress
            if epoch % 100 == 0:
                print(f"Epoch {epoch}: weights = {self.weights}, total error = {total_error}")

            # Early stopping if perfect classification
            if total_error == 0:
                print(f"Converged at epoch {epoch}")
                break


if __name__ == "__main__":
    # Input data
    X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])

    # AND problem
    y_and = np.array([0, 0, 0, 1])
    perceptron_and = Perceptron(num_features=2, learning_rate=0.1, num_epochs=1000)
    perceptron_and.train(X, y_and)

    print("\nAND function results:")
    for inputs in X:
        print(f"Input: {inputs}, Prediction: {perceptron_and.predict(inputs)}")

    # OR problem
    y_or = np.array([0, 1, 1, 1])
    perceptron_or = Perceptron(num_features=2, learning_rate=0.1, num_epochs=1000)
    perceptron_or.train(X, y_or)

    print("\nOR function results:")
    for inputs in X:
        print(f"Input: {inputs}, Prediction: {perceptron_or.predict(inputs)}")

    # XOR problem
    y_xor = np.array([0, 1, 1, 0])
    perceptron_xor = Perceptron(num_features=2, learning_rate=0.1, num_epochs=1000)
    perceptron_xor.train(X, y_xor)

    print("\nXOR function results:")
    for inputs in X:
        print(f"Input: {inputs}, Prediction: {perceptron_xor.predict(inputs)}")
</code></pre>
</div>
</div>
</body>
</html>